@inproceedings{aryaApproximateNearestNeighbor1993,
  title = {Approximate Nearest Neighbor Queries in Fixed Dimensions},
  booktitle = {Proceedings of the Fourth Annual {{ACM-SIAM}} Symposium on {{Discrete}} Algorithms},
  author = {Arya, Sunil and Mount, David M.},
  year = 1993,
  month = jan,
  series = {{{SODA}} '93},
  pages = {271--280},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {USA},
  urldate = {2025-10-22},
  isbn = {978-0-89871-313-8},
  langid = {american},
  file = {G:\Zotero_store\storage\6GCQHR6E\Arya and Mount - 1993 - Approximate nearest neighbor queries in fixed dimensions.pdf}
}

@misc{changThreeTipsGetting,
  title = {Three {{Tips}} for {{Getting More}} from {{Your Function Generator}} {\textbar} {{Keysight Blogs}}},
  author = {Chang, Choon-Hin},
  urldate = {2025-10-20},
  abstract = {This blog post will help you take advantage of the features of your function / arbitrary waveform generator so you can get your job done more quickly},
  howpublished = {https://www.keysight.com/blogs/en/tech/bench/2020/08/26/three-tips-for-getting-more-from-your-function-generator},
  langid = {english},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\WCHZELED\three-tips-for-getting-more-from-your-function-generator.html}
}

@misc{FundamentalsMOSFETsTheir,
  title = {Fundamentals of {{MOSFETs}} and {{Their Working}}},
  journal = {Components101},
  urldate = {2025-10-20},
  abstract = {Fundamentals of MOSFETs and Their Working},
  howpublished = {https://components101.com/articles/mosfet-symbol-working-operation-types-and-applications},
  langid = {english},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\EPK5LL92\mosfet-symbol-working-operation-types-and-applications.html}
}

@inproceedings{hauckPinAssignmentMultiFPGA1994,
  title = {Pin Assignment for Multi-{{FPGA}} Systems},
  booktitle = {Proceedings of {{IEEE Workshop}} on {{FPGA}}'s for {{Custom Computing Machines}}},
  author = {Hauck, S. and Borriello, G.},
  year = 1994,
  month = apr,
  pages = {11--13},
  doi = {10.1109/FPGA.1994.315593},
  urldate = {2025-10-25},
  abstract = {There is currently great interest in using systems of FPGAs for logic emulators, custom computing devices, and software accelerators. An important step in making these technologies more generally useful is to develop completely automatic mapping tools from high-level specifications to FPGA programming files. We examine one step in this automatic mapping process, the selection of FPGA pins to use for routing inter-FPGA signals. We present an algorithm that greatly increases mapping speed while also improving mapping quality.{$<>$}},
  langid = {american},
  keywords = {Circuit synthesis,Computer science,Field programmable gate arrays,Logic devices,Logic programming,Partitioning algorithms,Pins,Routing,Signal mapping,Wires},
  file = {G:\Zotero_store\storage\B3WM4RUN\Hauck and Borriello - 1994 - Pin assignment for multi-FPGA systems.pdf}
}

@misc{hristovIntroductionKDTrees2023,
  title = {Introduction to {{K-D Trees}} {\textbar} {{Baeldung}} on {{Computer Science}}},
  author = {Hristov, Hristo},
  year = 2023,
  month = mar,
  urldate = {2025-10-22},
  abstract = {Learn more about K-D Trees.},
  howpublished = {https://www.baeldung.com/cs/k-d-trees},
  langid = {american},
  file = {G:\Zotero_store\storage\ZEPFPHJX\k-d-trees.html}
}

@article{I2CbusSpecificationUser2021,
  title = {{{I2C-bus}} Specification and User Manual},
  year = 2021,
  volume = {2021},
  abstract = {Philips Semiconductors (now NXP Semiconductors) developed a simple bidirectional 2-wire bus for efficient inter-IC control, called the Inter-IC or I2Cbus. Only two bus lines are required: a serial data line (SDA) and a serial clock line (SCL). Serial, 8-bit oriented, bidirectional data transfers can be made at up to 100 kbit/s in Standard-mode, up to 400 kbit/s in Fast-mode, up to 1 Mbit/s in Fast-mode Plus (Fm+), or up to 3.4 Mbit/s in High-speed mode. Ultra Fast-mode is a unidirectional mode with data transfers of up to 5 Mbit/s.},
  langid = {english},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\983Z9W5E\2021 - I2C-bus specification and user manual.pdf}
}

@article{johnsonBillionScaleSimilaritySearch2021,
  title = {Billion-{{Scale Similarity Search}} with {{GPUs}}},
  author = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  year = 2021,
  month = jul,
  journal = {IEEE Transactions on Big Data},
  volume = {7},
  number = {3},
  pages = {535--547},
  issn = {2332-7790},
  doi = {10.1109/TBDATA.2019.2921572},
  urldate = {2025-10-08},
  abstract = {Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a novel design for k-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 {\texttimes} faster than prior GPU state of the art. It enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.},
  langid = {american},
  keywords = {Big Data,graphical processing units,Graphics processing units,Indexing,indexing methods,multimedia databases,Quantization (signal),Random access memory,Similarity search,Task analysis},
  file = {G:\Zotero_store\storage\MIVMUBZP\Johnson et al. - 2021 - Billion-Scale Similarity Search with GPUs.pdf}
}

@inproceedings{joProgressiveKdTree2017,
  title = {A Progressive K-d Tree for Approximate k-Nearest Neighbors},
  booktitle = {2017 {{IEEE Workshop}} on {{Data Systems}} for {{Interactive Analysis}} ({{DSIA}})},
  author = {Jo, Jaemin and Seo, Jinwook and Fekete, Jean-Daniel},
  year = 2017,
  month = oct,
  pages = {1--5},
  doi = {10.1109/DSIA.2017.8339084},
  urldate = {2025-10-22},
  abstract = {We present a progressive algorithm for approximate k-nearest neighbor search. Although the use of k-nearest neighbor libraries (KNN) is common in many data analysis methods, most KNN algorithms can only be run when the whole dataset has been indexed, i.e., they are not online. Even the few online implementations are not progressive in the sense that the time to index incoming data is not bounded and can exceed the latency required by progressive systems. This latency significantly restricts the interactivity of visualization systems especially when dealing with large-scale data. We improve traditional k-d trees for progressive approximate k-nearest neighbor search, enabling fast KNN queries while continuously indexing new batches of data when necessary. Following the progressive computation paradigm, our progressive k-d tree is bounded in time, allowing analysts to access ongoing results within an interactive latency. We also present performance benchmarks to compare online and progressive k-d trees.},
  langid = {american},
  keywords = {Algorithm,Approximate k-Nearest-Neighbors,Approximation algorithms,Buildings,Data analysis,Data structures,Indexing,Progressive Data Analysis,Real-Time,Vegetation},
  file = {G:\Zotero_store\storage\5KKHHHSX\Jo et al. - 2017 - A progressive k-d tree for approximate k-nearest neighbors.pdf}
}

@inproceedings{kanekoMapDatabaseDesign2016,
  title = {Map {{Database Design}} for {{Route Finding}} and {{Natural Disaster Risk Evaluation}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Agents}} ({{ICA}})},
  author = {Kaneko, Kunihiko},
  year = 2016,
  month = sep,
  pages = {139--141},
  doi = {10.1109/ICA.2016.048},
  urldate = {2025-10-13},
  abstract = {A map consists of layers. There are two types of layers of a map. They are raster and vector. A raster layer is a set of pixels, and a vector layer is a set of geometric entities. It is important to create a map layer from database contents dynamically. However, it is still a problem to create map layers easily, and to manage the relationship among created layers and database contents. In this paper, the author proposes to extend raster layer and vector layer. First, a new type of vector layer is proposed to make it enable to aggregate objects into a vector layer. Second, a new type of raster layer is introduced to make it enable to create a raster layer dynamically. The extensions are implemented on top of relational database management system. They are evaluated using two scenarios, routes finding and natural disaster risk evaluation.},
  keywords = {/unread,Aggregates,Conferences,Database systems,database view,Geometry,map database,raster layer,relational database,Relational databases,Spatial databases,vector layer},
  file = {G:\Zotero_store\storage\VXHEKWDM\Kaneko - 2016 - Map Database Design for Route Finding and Natural Disaster Risk Evaluation.pdf}
}

@misc{maGraphBasedApproximateNearest2025,
  title = {Graph-{{Based Approximate Nearest Neighbor Search Revisited}}: {{Theoretical Analysis}} and {{Optimization}}},
  shorttitle = {Graph-{{Based Approximate Nearest Neighbor Search Revisited}}},
  author = {Ma, Xinran and Zhou, Zhaoqi and Zhou, Chuan and Meng, Qi and Shang, Zaijiu and Li, Guoliang and Ma, Zhiming},
  year = 2025,
  month = sep,
  number = {arXiv:2509.15531},
  eprint = {2509.15531},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.15531},
  urldate = {2025-10-15},
  abstract = {Graph-based approaches to approximate nearest neighbor search (ANNS) have achieved remarkable success in enabling fast, high-recall retrieval on billion-scale vector datasets. Among them, the Sparse Neighborhood Graph (SNG) has emerged as a widely adopted graph structure due to its superior search performance. However, the theoretical understanding of SNG remains limited, leading to reliance on heuristic-based and often suboptimal truncation strategies. In this work, we aim to bridge the gap between theory and practice by providing formal guarantees for graph-based ANNS methods and proposing principled optimization strategies for the truncation parameter. By characterizing the index construction process through martingale-based analysis, we show that the degree of the index graph is \$O(n{\textasciicircum}\{2/3+{\textbackslash}epsilon\})\$, where \${\textbackslash}epsilon\$ is an arbitrarily small constant. Furthermore, we prove that the expected search path length during query processing is \$O({\textbackslash}log n)\$. Based on these theoretical insights, we introduce a novel and principled method for selecting the truncation parameter \$R\$ in SNG. Experimental results demonstrate that our method achieves comparable or superior performance in terms of query latency and Recall@10 compared to commonly used binary search heuristics, while yielding 2x to 9x speedups in overall index construction.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {G\:\\Zotero_store\\storage\\243FQAT6\\Ma et al. - 2025 - Graph-Based Approximate Nearest Neighbor Search Revisited Theoretical Analysis and Optimization.pdf;G\:\\Zotero_store\\storage\\G3NEKYWV\\2509.html}
}

@article{malkovEfficientRobustApproximate2020,
  title = {Efficient and {{Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs}}},
  author = {Malkov, Yu A. and Yashunin, D. A.},
  year = 2020,
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {42},
  number = {4},
  pages = {824--836},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2889473},
  urldate = {2025-10-16},
  abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures (typically used at the coarse search stage of the most proximity graph techniques). Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting the search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
  langid = {american},
  keywords = {approximate search,Approximation algorithms,artificial intelligence,big data,Biological system modeling,Brain modeling,Complexity theory,Data models,data structures,Graph and tree search strategies,graphs and networks,information search and retrieval,information storage and retrieval,information technology and systems,nearest neighbor search,Routing,Search problems,search process,similarity search},
  file = {G:\Zotero_store\storage\VWH49NZY\Malkov and Yashunin - 2020 - Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Gr.pdf}
}

@misc{manglaImplementingApproximateNearest2024,
  title = {Implementing {{Approximate Nearest Neighbor Search}} with {{KD-Trees}}},
  author = {Mangla, Puneet},
  year = 2024,
  month = dec,
  journal = {PyImageSearch},
  urldate = {2025-10-22},
  abstract = {Unlock efficient data searching with KD-Trees! Learn to implement Approximate Nearest Neighbor Search for faster, accurate results in large datasets.},
  langid = {american},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\TGZUZZV5\implementing-approximate-nearest-neighbor-search-with-kd-trees.html}
}

@misc{MicrostripCapacitance,
  title = {Microstrip {{Capacitance}}},
  journal = {Electromagnetic Interference Software -- EMI Software LLC},
  urldate = {2025-10-20},
  abstract = {Capacitance calculator for microstrip conductors. Calculates the capacitance of microstrip conductors embedded in a homogeneous dielectric medium.},
  langid = {american},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\F9B94XUP\microstrip-capacitance.html}
}

@misc{mitraAnalogueElectronicsProject2024,
  title = {Analogue {{Electronics}} ({{Project}}) 4, {{VLSI Lab}}},
  author = {Mitra, Srinjoy},
  year = 2024,
  langid = {english},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\A8ECGMMR\Mitra - 2024 - Analogue Electronics (Project) 4, VLSI Lab.pdf}
}

@misc{MOSFET,
  title = {The {{MOSFET}}},
  urldate = {2025-10-20},
  howpublished = {https://www.talkingelectronics.com/projects/MOSFET/MOSFET.html},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\Z6YQSP3C\MOSFET.html}
}

@book{razaviDesignAnalogCMOS2017,
  title = {Design of Analog {{CMOS}} Integrated Circuits},
  author = {Razavi, Behzad},
  year = 2017,
  edition = {2},
  publisher = {McGraw-Hill Education},
  address = {New York, NY},
  isbn = {978-0-07-252493-2 978-981-4636-26-1},
  langid = {english},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\U7B58LIB\Razavi - 2017 - Design of analog CMOS integrated circuits.pdf}
}

@misc{schmidtFETsFieldEffectTransistors2021,
  title = {{{FETs}} ({{Field-Effect Transistors}}) - {{Education}}},
  author = {Schmidt, Alec},
  year = 2021,
  month = mar,
  journal = {DigiKey TechForum - An Electronic Component and Engineering Solution Forum},
  urldate = {2025-10-25},
  abstract = {A FET is a three terminal semiconductor device in which the voltage across its gate and source terminals is used to control the current flow between its drain and source terminals. This behavior allows a FET to act as a voltage controlled resistance, amplifier, or switch, and this makes them applicable to both analog and digital electronics.  There are many different types of FETs available, and each type has its own specific variation of materials, configuration, and geometric arrangement. On t...},
  chapter = {Education},
  howpublished = {https://forum.digikey.com/t/fets-field-effect-transistors/13119},
  langid = {english},
  file = {G:\Zotero_store\storage\C3FL5N5E\13119.html}
}

@inproceedings{sharmaIntegrationVectorDatabases2025,
  title = {Integration of {{Vector Databases}} with {{Large Language Models}} ({{LLMs}})},
  booktitle = {2025 {{International Conference}} on {{Next Generation Information System Engineering}} ({{NGISE}})},
  author = {Sharma, Palak and Pundir, Ayushman Singh and Singh, Gagan Dev and Gupta, Ruchi},
  year = 2025,
  month = mar,
  volume = {1},
  pages = {1--5},
  doi = {10.1109/NGISE64126.2025.11085087},
  urldate = {2025-10-13},
  abstract = {The fast-accelerating growth of unstructured and semi-structured data leads traditional relational and NoSQL databases to unsatisfactory performance of today's queries system.This paper explores the advantages of using vector databases rather than using traditional databases for storing and maintaining the databases with Large Language Models(LLMs) to improve and increase the efficiency of the query retrieval.Vector embeddings are generated using Hugging Face models, while Google PaLM further refines the search process. To efficiently match queries, Locality Sensitive Hashing (LSH) is used. The results significantly demonstrates the improvements in query accuracy and retrieval speed, underlining the collaboration between vector databases and LLMs for data-intensive applications.},
  keywords = {/unread,Artificial intelligence,Collaboration,Data models,Databases,Faces,Hugging Face,Large language models,LLMs,Locality Sensitive Hashing,Next generation networking,Scalability,Semantics,Technological innovation,vector databases,Vectors},
  file = {G:\Zotero_store\storage\6M95CQN8\Sharma et al. - 2025 - Integration of Vector Databases with Large Language Models (LLMs).pdf}
}

@inproceedings{sharmaIntegrationVectorDatabases2025a,
  title = {Integration of {{Vector Databases}} with {{Large Language Models}} ({{LLMs}})},
  booktitle = {2025 {{International Conference}} on {{Next Generation Information System Engineering}} ({{NGISE}})},
  author = {Sharma, Palak and Pundir, Ayushman Singh and Singh, Gagan Dev and Gupta, Ruchi},
  year = 2025,
  month = mar,
  volume = {1},
  pages = {1--5},
  doi = {10.1109/NGISE64126.2025.11085087},
  urldate = {2025-10-06},
  abstract = {The fast-accelerating growth of unstructured and semi-structured data leads traditional relational and NoSQL databases to unsatisfactory performance of today's queries system.This paper explores the advantages of using vector databases rather than using traditional databases for storing and maintaining the databases with Large Language Models(LLMs) to improve and increase the efficiency of the query retrieval.Vector embeddings are generated using Hugging Face models, while Google PaLM further refines the search process. To efficiently match queries, Locality Sensitive Hashing (LSH) is used. The results significantly demonstrates the improvements in query accuracy and retrieval speed, underlining the collaboration between vector databases and LLMs for data-intensive applications.},
  langid = {american},
  keywords = {Artificial intelligence,Collaboration,Data models,Databases,Faces,Hugging Face,Large language models,LLMs,Locality Sensitive Hashing,Next generation networking,Scalability,Semantics,Technological innovation,vector databases,Vectors},
  file = {G:\Zotero_store\storage\FQICBSQX\Sharma et al. - 2025 - Integration of Vector Databases with Large Language Models (LLMs).pdf}
}

@inproceedings{singhAnalyzingEmbeddingModels2023,
  title = {Analyzing {{Embedding Models}} for {{Embedding Vectors}} in {{Vector Databases}}},
  booktitle = {2023 {{IEEE International Conference}} on {{ICT}} in {{Business Industry}} \& {{Government}} ({{ICTBIG}})},
  author = {Singh, Paras Nath and Talasila, Sreya and Banakar, Shivaraj Veerappa},
  year = 2023,
  month = dec,
  pages = {1--7},
  doi = {10.1109/ICTBIG59752.2023.10455990},
  urldate = {2025-10-13},
  abstract = {Vector databases have emerged as the computation engine that enables us to successfully interact with vector embeddings in our applications as a result of the exponential rise of vector embeddings in disciplines like NLP (Natural Language Processing), computer vision, and other AI applications. In order to address the issues that arise when handling vector embeddings in production applications, vector databases were specifically created. Vector databases that offer quick and precise nearest-neighbor search, clustering, and similarity matching, and that are simple to deploy on cloud infrastructure or distributed computing systems, are more likely to be well-liked by users. They therefore provide a number of advantages over conventional scalar-based databases and independent vector indexes. This research reveals that embedding vectors are frequently utilized for analyzing and exploring unstructured data with the creation of learning-based embedding models. Completely managed and horizontally scalable vector databases are required as vector collections reach billion-scale numbers. The proposal relaxes the data model and consistency restrictions in exchange for the aforementioned benefits because the majority of vector data applications do not call for intricate data models and robust data consistency. VectorDB of Python has been used for implementation and test case which does faster similarity search.},
  keywords = {/unread,Analytical models,Artificial Intelligence,Behavioral sciences,Data models,Databases,Density estimates,Face recognition,Natural language processing,Nearest Neighbor,Neural Networks,Vector databases,Vector Embeddings,Vector indices,VectorDB,Vectors},
  file = {G:\Zotero_store\storage\CWD9AFPC\Singh et al. - 2023 - Analyzing Embedding Models for Embedding Vectors in Vector Databases.pdf}
}

@article{songEfficientFPGAImplementation2025,
  title = {An {{Efficient FPGA Implementation}} of {{Approximate Nearest Neighbor Search}}},
  author = {Song, Yifeng and Liu, Chenjie and Zhang, Rongrong and Zhu, Danyang and Wang, Zhongfeng},
  year = 2025,
  month = jun,
  journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume = {33},
  number = {6},
  pages = {1705--1714},
  issn = {1557-9999},
  doi = {10.1109/TVLSI.2025.3544342},
  urldate = {2025-10-06},
  abstract = {Approximate nearest neighbor search (ANNS) plays an important role in modern artificial intelligence (AI) systems, being extensively utilized in search engines, advertising, and recommendation systems. With the advent of large language models (LLMs), ANNS is increasingly finding applications in edge scenarios such as personal assistants. The demand for efficient and fast ANNS solutions is, therefore, more pressing than ever. In this article, we propose a scalable and efficient field-programmable gate array (FPGA) implementation of ANNS based on the inverted file with product quantization (IVF-PQ) algorithm, thus marking the first hardware implementation supporting up to 1024-D datasets. First, we devise a novel architecture for the Top-K module, capable of processing multiple input data streams simultaneously and linearly increasing throughput. Second, we adjust the data precision in several parts of our design, thus achieving obvious performance improvement without losing much recall. Moreover, we introduce a flexible distance calculation (Distance Cal) module that can be reused for various computational tasks at different query stages. We code our design in Verilog and implement it on Xilinx Alveo U280. The experimental results show that our search latency can be as low as 0.0071 ms at a 94\% recall, while the power is 19.80 W. Compared to the state-of-the-art application-specified integrated circuit (ASIC) implementations, our design delivers a 4.5{\textbackslash}times speedup in latency and a 20\% reduction in energy consumption.},
  langid = {american},
  keywords = {Approximate nearest neighbor search (ANNS),Approximation algorithms,Artificial intelligence,Clustering algorithms,Codes,Databases,Field programmable gate arrays,field-programmable gate array (FPGA),hardware,Hardware,Indexes,inverted file with product quantization (IVF-PQ),Quantization (signal),Vectors},
  file = {G:\Zotero_store\storage\UBWQ9AJQ\Song et al. - 2025 - An Efficient FPGA Implementation of Approximate Nearest Neighbor Search.pdf}
}

@misc{SPIVsI2C2024,
  title = {{{SPI}} vs {{I2C}}: {{Comparing Speed}}, {{Pins}}, and {{Data Transfer Methods}}},
  shorttitle = {{{SPI}} vs {{I2C}}},
  year = 2024,
  month = jul,
  urldate = {2025-10-20},
  abstract = {SPI vs I2C, how to choose the right communication protocol? Compare full-duplex speed, 2-wire interface, slave addressing, and hardware complexity to select the best bus for your application.},
  chapter = {Ordering Assistance},
  howpublished = {https://www.fs-pcba.com/spi-vs-i2c/},
  langid = {american},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\VTX7DCCC\spi-vs-i2c.html}
}

@inproceedings{srishwalSimilaritySearchVector2024,
  title = {Similarity {{Search}} and {{Vector Databases}} for {{Heart Arrhythmia Classification}}},
  booktitle = {2024 4th {{International Conference}} on {{Ubiquitous Computing}} and {{Intelligent Information Systems}} ({{ICUIS}})},
  author = {Srishwal, Dhruvi and PS, Shibu and Vardan, Harsha and A, Sajjad Ibrahim. and P, {\relax Gouthaman}.},
  year = 2024,
  month = dec,
  pages = {411--415},
  doi = {10.1109/ICUIS64676.2024.10866154},
  urldate = {2025-10-06},
  abstract = {Heart arrhythmias, if not detected early, can lead to severe conditions like stroke or heart failure, making timely and accurate classification critical. Traditional methods of analyzing Electrocardiogram (ECG) data are manual, time-consuming, and prone to errors due to the complexity of ECG signals. This paper presents an automated approach for arrhythmia classification using vector databases and similarity search algorithms. By converting raw ECG time-series data into vectorized representations, the system enables efficient pattern recognition and real-time analysis. Machine learning models applied to the vectorized data enhance classification accuracy, while the use of a vector database allows for quick similarity searches, facilitating real-time comparison of patient data with known arrhythmia patterns. The proposed method significantly reduces analysis time and increases precision, offering a robust framework for real-time monitoring and early detection, thereby improving patient outcomes and enabling timely medical interventions.},
  keywords = {/unread,Accuracy,Approximate Similarity Search,Arrhythmia,Automated classification,Clustering,Databases,Dimensionality Reduction,early detection. Support Vector Machine,Electrocardiogram (ECG),Electrocardiography,Heart,Heart arrhythmia,K Nearest Neighbor,Machine learning,Monitoring,Multi-class Classification,Pattern recognition,Principal Component Analysis,Real-time monitoring,Real-time systems,Similarity Search,Time2Vec,Ubiquitous computing,vector database,Vectorization,Vectors},
  file = {G:\Zotero_store\storage\9PUJWYEV\Srishwal et al. - 2024 - Similarity Search and Vector Databases for Heart Arrhythmia Classification.pdf}
}

@misc{stoyanovOpenDrainOutput2019,
  title = {Open {{Drain Output}} vs. {{Push-Pull Output}}},
  author = {Stoyanov, Yasen},
  year = 2019,
  month = mar,
  journal = {Open4Tech},
  urldate = {2025-10-20},
  abstract = {In open drain configuration, the logic behind the pin can drive it only to ground (logic 0). The other possible state is high impedance (Hi-Z). Push-pull output is capable of driving two output levels (logic 1 and logic 0)},
  langid = {american},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\YYHRTAS4\open-drain-output-vs-push-pull-output.html}
}

@misc{teamDifferenceMicrostripStripline2021,
  title = {Difference {{Between Microstrip}} and {{Stripline}}},
  author = {Team, The Sierra},
  year = 2021,
  month = feb,
  journal = {Sierra Circuits},
  urldate = {2025-10-20},
  abstract = {Microstrip and stripline have distinct EM field distributions. Dielectric constant and loss tangent are vital to achieve controlled impedance},
  langid = {american},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\6RS6J274\difference-between-microstrip-stripline-pcb.html}
}

@misc{TerminationOpenDrain,
  title = {Termination of {{Open Drain Output Pins}}},
  urldate = {2025-10-20},
  howpublished = {https://microchip.my.site.com/s/article/Termination-of-Open-Drain-Output-Pins},
  keywords = {/unread},
  file = {G:\Zotero_store\storage\J5BDEE8K\Termination-of-Open-Drain-Output-Pins.html}
}

@article{vemaROLEVECTORDATABASES2024,
  title = {{{ROLE OF VECTOR DATABASES IN LARGE LANGUAGE MODELS}} ({{LLMS}})},
  author = {Vema, Narendra Nadh},
  year = 2024,
  month = jun,
  journal = {INTERNATIONAL JOURNAL OF CREATIVE RESEARCH THOUGHTS},
  volume = {12},
  pages = {i436-i474},
  abstract = {This Artificial intelligence has been revolutionized due to the integration the vector data with LLMs and enhancing the similarity search as well as boosting semantic analysis. Vector databases have a lot of benefits but besides the benefits, they also face challenges that can reduce the performance of the LLMs. To deal with the challenges faced by vector databases there is a need for innovative technologies that can boost the responses. Organizations can enhance innovations in the applications of language processing and can provide the path for further advancements.},
  langid = {american},
  file = {G:\Zotero_store\storage\AU8BD5CA\Vema - 2024 - ROLE OF VECTOR DATABASES IN LARGE LANGUAGE MODELS (LLMS).pdf}
}

@inproceedings{wangDCNV2Improved2021,
  title = {{{DCN V2}}: {{Improved Deep}} \& {{Cross Network}} and {{Practical Lessons}} for {{Web-scale Learning}} to {{Rank Systems}}},
  shorttitle = {{{DCN V2}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Wang, Ruoxi and Shivanna, Rakesh and Cheng, Derek Z. and Jain, Sagar and Lin, Dong and Hong, Lichan and Chi, Ed H.},
  year = 2021,
  month = apr,
  eprint = {2008.13535},
  primaryclass = {cs},
  pages = {1785--1797},
  doi = {10.1145/3442381.3450078},
  urldate = {2025-10-08},
  abstract = {Learning effective feature crosses is the key behind building recommender systems. However, the sparse and large feature space requires exhaustive search to identify effective crosses. Deep \& Cross Network (DCN) was proposed to automatically and efficiently learn bounded-degree predictive feature interactions. Unfortunately, in models that serve web-scale traffic with billions of training examples, DCN showed limited expressiveness in its cross network at learning more predictive feature interactions. Despite significant research progress made, many deep learning models in production still rely on traditional feed-forward neural networks to learn feature crosses inefficiently. In light of the pros/cons of DCN and existing feature interaction learning approaches, we propose an improved framework DCN-V2 to make DCN more practical in large-scale industrial settings. In a comprehensive experimental study with extensive hyper-parameter search and model tuning, we observed that DCN-V2 approaches outperform all the state-of-the-art algorithms on popular benchmark datasets. The improved DCN-V2 is more expressive yet remains cost efficient at feature interaction learning, especially when coupled with a mixture of low-rank architecture. DCN-V2 is simple, can be easily adopted as building blocks, and has delivered significant offline accuracy and online business metrics gains across many web-scale learning to rank systems at Google.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G\:\\Zotero_store\\storage\\5RKH3G89\\Wang et al. - 2021 - DCN V2 Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems.pdf;G\:\\Zotero_store\\storage\\LUYTDQIW\\2008.html}
}

@misc{WhatVectorDatabase2024,
  title = {What {{Is A Vector Database}}? {\textbar} {{IBM}}},
  shorttitle = {What {{Is A Vector Database}}?},
  year = 2024,
  month = jul,
  urldate = {2025-10-08},
  abstract = {A vector database stores, manages and indexes high-dimensional vector data to be stored as arrays of numbers called ``vectors,'' clustered based on similarity.},
  howpublished = {https://www.ibm.com/think/topics/vector-database},
  langid = {english},
  file = {G:\Zotero_store\storage\2XZ5DJGE\vector-database.html}
}

@article{yuanFANNSFPGABasedApproximate2025,
  title = {{{FANNS}}: {{An FPGA-Based Approximate Nearest-Neighbor Search Accelerator}}},
  shorttitle = {{{FANNS}}},
  author = {Yuan, Wei and Jin, Xi},
  year = 2025,
  month = apr,
  journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume = {33},
  number = {4},
  pages = {1197--1201},
  issn = {1557-9999},
  doi = {10.1109/TVLSI.2024.3496589},
  urldate = {2025-10-08},
  abstract = {Approximate nearest-neighbor search (ANNS) based on high-dimensional vectors has been extensively utilized in data science and neural networks. However, deploying ANNS in production systems requires minimal redundant computation, high recall rates, and low on-chip memory usage, which existing hardware accelerators fail to offer. We propose FANNS, a solution for ANNS based on high-dimensional vectors that can eliminate redundant computations and reuse on-chip data. Extensive evaluations show that FANNS achieves an average of 184.1{\textbackslash}times , 33.0{\textbackslash}times , 2.9{\textbackslash}times , and 2.5{\textbackslash}times better energy efficiency than CPUs, GPUs, and two state-of-the-art ANNS architectures, i.e., DF-GAS and Vstore, respectively.},
  langid = {american},
  keywords = {Approximate nearest-neighbor search (ANNS),Bandwidth,Computer architecture,Data transfer,Databases,Energy efficiency,Field programmable gate arrays,field-programmable gate array (FPGA),Graphics processing units,hardware accelerator,hardware architecture,Nearest neighbor methods,Sorting,Vectors},
  file = {G:\Zotero_store\storage\ND9YSBS4\Yuan and Jin - 2025 - FANNS An FPGA-Based Approximate Nearest-Neighbor Search Accelerator.pdf}
}

@inproceedings{yuSimilaritySearchGraph2020,
  title = {Similarity Search of Graph Database Based on {{Fuzzy Logic Support}} Vector Machine ({{PSO-SVM}}) Algorithm and Computer Application},
  booktitle = {2020 2nd {{International Conference}} on {{Information Technology}} and {{Computer Application}} ({{ITCA}})},
  author = {Yu, Tao and He, Fengqin},
  year = 2020,
  month = dec,
  pages = {97--100},
  doi = {10.1109/ITCA52113.2020.00027},
  urldate = {2025-10-13},
  abstract = {Support vector machine (SVM) has excellent learning performance, has become a popular research method in the field of machine learning, and has been successfully applied in many fields. In recent years, more and more modeling methods have been put forward by relevant scholars to solve problems such as classification identification, risk prediction, and effectiveness evaluation. This article briefly introduces the support vector machine (SVM), and expounds the algorithm of support vector machine in graph database similarity search and computer application for readers' reference.},
  keywords = {/unread,Artificial Intelligence (Ai),Computer Application,Computer applications,Databases,Graph Database,Image retrieval,Machine learning algorithms,Neural networks,Predictive models,Support Vector Machine,Support vector machines},
  file = {G:\Zotero_store\storage\YGW2L8T4\Yu and He - 2020 - Similarity search of graph database based on Fuzzy Logic Support vector machine (PSO-SVM) algorithm.pdf}
}

@inproceedings{zengDFGASDistributedFPGAasaService2023,
  title = {{{DF-GAS}}: A {{Distributed FPGA-as-a-Service Architecture}} towards {{Billion-Scale Graph-based Approximate Nearest Neighbor Search}}},
  shorttitle = {{{DF-GAS}}},
  booktitle = {2023 56th {{IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Zeng, Shulin and Zhu, Zhenhua and Liu, Jun and Zhang, Haoyu and Dai, Guohao and Zhou, Zixuan and Li, Shuangchen and Ning, Xuefei and Xie, Yuan and Yang, Huazhong and Wang, Yu},
  year = 2023,
  month = oct,
  pages = {283--296},
  urldate = {2025-10-08},
  abstract = {Embedding retrieval is a crucial task for recommendation systems. Graph-based approximate nearest neighbor search (GANNS) is the most commonly used method for retrieval, and achieves the best performance on billion-scale datasets. Unfortunately, the existing CPU- and GPU-based GANNS systems are difficult to optimize the throughput under the latency constraints on billion-scale datasets, due to the underutilized local memory bandwidth (5-45\%) and the expensive remote data access overhead ({$\sim$}85\% of the total latency). In this paper, we first introduce a practically ideal GANNS architecture for billion-scale datasets, which facilitates a detailed analysis of the challenges and characteristics of distributed GANNS systems. Then, at the architecture level, we propose DF-GAS, a Distributed FPGA-as-a-Service (FPaaS) architecture for accelerating billion-scale Graph-based Approximate nearest neighbor Search. DF-GAS uses a feature-packing memory access engine and a data prefetching and delayed processing scheme to increase local memory bandwidth by 36-42\% and reduce remote data access overhead by 76.2\%, respectively. At the system level, we exploit the "full-graph + sub-graph" hybrid parallel search scheme on distributed FPaaS system. It achieves million-level query-per-second with sub-millisecond latency on billion-scale GANNS for the first time. Extensive evaluations on million-scale and billion-scale datasets show that DF-GAS achieves an average of 55.4{\texttimes}, 32.2{\texttimes}, 5.4{\texttimes}, and 4.4{\texttimes} better latency-bounded throughput than CPUs, GPUs, and two state-of-the-art ANNS architectures, i.e., ANNA [23] and Vstore [27], respectively.CCS CONCEPTS{$\bullet$} Computer systems organization {$\rightarrow$} Cloud computing.},
  langid = {american},
  keywords = {Approximate Nearest Neighbor Search,Bandwidth,Computer architecture,Distributed Architecture,Embedding Retrieval,Field programmable analog arrays,FPGA,Graph,Nearest neighbor methods,Recommender systems,Task analysis,Throughput},
  file = {G:\Zotero_store\storage\95RY8ILX\Zeng et al. - 2023 - DF-GAS a Distributed FPGA-as-a-Service Architecture towards Billion-Scale Graph-based Approximate N.pdf}
}

@inproceedings{zhangAreThereFundamental2024,
  title = {Are {{There Fundamental Limitations}} in {{Supporting Vector Data Management}} in {{Relational Databases}}? {{A Case Study}} of {{PostgreSQL}}},
  shorttitle = {Are {{There Fundamental Limitations}} in {{Supporting Vector Data Management}} in {{Relational Databases}}?},
  booktitle = {2024 {{IEEE}} 40th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Zhang, Yunan and Liu, Shige and Wang, Jianguo},
  year = 2024,
  month = may,
  pages = {3640--3653},
  issn = {2375-026X},
  doi = {10.1109/ICDE60146.2024.00280},
  urldate = {2025-10-13},
  abstract = {High-dimensional vector data is gaining increasing importance in data science applications. Consequently, various database systems have recently been developed to manage vector data. These systems can be broadly categorized into two types: specialized and generalized vector databases. Specialized vector databases are explicitly designed and optimized for storing and querying vector data, while generalized vector databases support vector data management within a relational database like PostgreSQL. It is expected (and confirmed by our experiments) that generalized vector databases exhibit slower performance. However, it is not clear whether there are fundamental limitations (or just implementation issues) for relational databases to support vector data management. This paper aims to answer this question. We chose PostgreSQL as a representative relational database due to its popularity. We focused on PASE, as it is a high-performance and open-sourced PostgreSQL-based vector database. We analyzed the source code of PASE and compared its performance with Faiss, a high-performance and open-sourced specialized vector database, to identify the underlying root causes of the performance gap and analyze how to bridge the gap. Based on our results, we provide insights and directions for building a future generalized vector database that can achieve comparable performance to a high-performance specialized vector database.},
  keywords = {/unread,Buildings,Data engineering,Data science,Databases,Generalized Vector Databases,Relational databases,Source coding,Specialized Vector Databases,Vector Databases,Vector Similarity Search,Vectors},
  file = {G:\Zotero_store\storage\JBDVE88N\Zhang et al. - 2024 - Are There Fundamental Limitations in Supporting Vector Data Management in Relational Databases A Ca.pdf}
}
